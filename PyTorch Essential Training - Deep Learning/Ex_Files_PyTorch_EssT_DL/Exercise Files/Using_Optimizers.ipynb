{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using_Optimizers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBehvqZ8znsy",
        "colab_type": "text"
      },
      "source": [
        "# Using optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCdIqY0tKbvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting seeds to try and ensure we have the same results - this is not guaranteed across PyTorch releases.\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PCJzXv0OK1Bs",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "mean, std = (0.5,), (0.5,)\n",
        "\n",
        "# Create a transform and normalise data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                              ])\n",
        "\n",
        "# Download FMNIST training dataset and load training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download FMNIST test dataset and load test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZpZ12MrEDZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqMqFbIVrbFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FMNIST(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "    self.fc2 = nn.Linear(128,64)\n",
        "    self.fc3 = nn.Linear(64,10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    \n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    \n",
        "    return x\n",
        "    \n",
        "#model = FMNIST()   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m68OeMRdEF0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c0QgxCF3fD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjBut_7lhAc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2ZAGFzFEQA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iPQek2nz2yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMnVwV-CERd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roihp-kN0Jw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvbHIyPSEUPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtP3nCEQEUMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwcPkxQwEfYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nf2WdmP5Gst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "9419b8d7-5844-447d-eee6-acd48c9723ca"
      },
      "source": [
        "output = model(images)\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0286,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[-0.0004, -0.0004, -0.0004,  ..., -0.0007, -0.0006, -0.0004],\n",
            "        [ 0.0069,  0.0069,  0.0069,  ...,  0.0072,  0.0070,  0.0069],\n",
            "        [-0.0015, -0.0015, -0.0015,  ..., -0.0016, -0.0015, -0.0015],\n",
            "        ...,\n",
            "        [ 0.0018,  0.0018,  0.0018,  ...,  0.0017,  0.0017,  0.0018],\n",
            "        [ 0.0019,  0.0019,  0.0019,  ...,  0.0019,  0.0019,  0.0019],\n",
            "        [ 0.0017,  0.0017,  0.0017,  ...,  0.0016,  0.0017,  0.0017]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arwzAK-1EkEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use the optimizer to update the weights\n",
        "optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD-u49yzEj6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuGKi_nq6P0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "dce144d9-91cd-47db-923c-05561851e87d"
      },
      "source": [
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0151, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0296,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0221, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0285,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[-0.0004, -0.0004, -0.0004,  ..., -0.0007, -0.0006, -0.0004],\n",
            "        [ 0.0069,  0.0069,  0.0069,  ...,  0.0072,  0.0070,  0.0069],\n",
            "        [-0.0015, -0.0015, -0.0015,  ..., -0.0016, -0.0015, -0.0015],\n",
            "        ...,\n",
            "        [ 0.0018,  0.0018,  0.0018,  ...,  0.0017,  0.0017,  0.0018],\n",
            "        [ 0.0019,  0.0019,  0.0019,  ...,  0.0019,  0.0019,  0.0019],\n",
            "        [ 0.0017,  0.0017,  0.0017,  ...,  0.0016,  0.0017,  0.0017]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8oIy5SkEpDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to zero out gradients to avoid accumulation\n",
        "optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnfpzGigEpAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EniqxHDwDa8d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "fcfc6080-b905-4d62-cf92-4d7b3ca2c4d5"
      },
      "source": [
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0151, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0296,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0221, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0285,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DViAViGEwyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put everything on a loop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGZhQE3tDcqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc794405-bf62-48c4-c3c9-c205694cd771"
      },
      "source": [
        "model = FMNIST()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    cum_loss = 0\n",
        "    batch_num = 0\n",
        "\n",
        "    for batch_num, (images, labels) in enumerate(trainloader, 1):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        cum_loss += loss.item()\n",
        "        print(f'Batch : {batch_num}, Loss : {loss.item()}')\n",
        "     \n",
        "    print(f\"Training loss: {cum_loss/len(trainloader)}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch : 1, Loss : 2.3072664737701416\n",
            "Batch : 2, Loss : 2.288365125656128\n",
            "Batch : 3, Loss : 2.3049063682556152\n",
            "Batch : 4, Loss : 2.3015007972717285\n",
            "Batch : 5, Loss : 2.278282403945923\n",
            "Batch : 6, Loss : 2.277097702026367\n",
            "Batch : 7, Loss : 2.2820229530334473\n",
            "Batch : 8, Loss : 2.302107810974121\n",
            "Batch : 9, Loss : 2.2785263061523438\n",
            "Batch : 10, Loss : 2.278015375137329\n",
            "Batch : 11, Loss : 2.2610421180725098\n",
            "Batch : 12, Loss : 2.2764647006988525\n",
            "Batch : 13, Loss : 2.2731387615203857\n",
            "Batch : 14, Loss : 2.2578248977661133\n",
            "Batch : 15, Loss : 2.2600483894348145\n",
            "Batch : 16, Loss : 2.261315107345581\n",
            "Batch : 17, Loss : 2.2721023559570312\n",
            "Batch : 18, Loss : 2.24147629737854\n",
            "Batch : 19, Loss : 2.2405261993408203\n",
            "Batch : 20, Loss : 2.235086679458618\n",
            "Batch : 21, Loss : 2.2544260025024414\n",
            "Batch : 22, Loss : 2.2362048625946045\n",
            "Batch : 23, Loss : 2.231415271759033\n",
            "Batch : 24, Loss : 2.2389469146728516\n",
            "Batch : 25, Loss : 2.2141687870025635\n",
            "Batch : 26, Loss : 2.233987331390381\n",
            "Batch : 27, Loss : 2.2178635597229004\n",
            "Batch : 28, Loss : 2.230189323425293\n",
            "Batch : 29, Loss : 2.2189369201660156\n",
            "Batch : 30, Loss : 2.2120747566223145\n",
            "Batch : 31, Loss : 2.212344169616699\n",
            "Batch : 32, Loss : 2.208530902862549\n",
            "Batch : 33, Loss : 2.18330717086792\n",
            "Batch : 34, Loss : 2.1965603828430176\n",
            "Batch : 35, Loss : 2.181426763534546\n",
            "Batch : 36, Loss : 2.2011210918426514\n",
            "Batch : 37, Loss : 2.196010112762451\n",
            "Batch : 38, Loss : 2.159069061279297\n",
            "Batch : 39, Loss : 2.1497068405151367\n",
            "Batch : 40, Loss : 2.1899259090423584\n",
            "Batch : 41, Loss : 2.164797306060791\n",
            "Batch : 42, Loss : 2.176614761352539\n",
            "Batch : 43, Loss : 2.15094256401062\n",
            "Batch : 44, Loss : 2.1727468967437744\n",
            "Batch : 45, Loss : 2.149967670440674\n",
            "Batch : 46, Loss : 2.171332359313965\n",
            "Batch : 47, Loss : 2.1695690155029297\n",
            "Batch : 48, Loss : 2.1276767253875732\n",
            "Batch : 49, Loss : 2.138857126235962\n",
            "Batch : 50, Loss : 2.119607925415039\n",
            "Batch : 51, Loss : 2.155275583267212\n",
            "Batch : 52, Loss : 2.106316328048706\n",
            "Batch : 53, Loss : 2.1626837253570557\n",
            "Batch : 54, Loss : 2.143470287322998\n",
            "Batch : 55, Loss : 2.122385025024414\n",
            "Batch : 56, Loss : 2.1170125007629395\n",
            "Batch : 57, Loss : 2.11470627784729\n",
            "Batch : 58, Loss : 2.1105237007141113\n",
            "Batch : 59, Loss : 2.1023786067962646\n",
            "Batch : 60, Loss : 2.1150588989257812\n",
            "Batch : 61, Loss : 2.1015803813934326\n",
            "Batch : 62, Loss : 2.099888324737549\n",
            "Batch : 63, Loss : 2.090914726257324\n",
            "Batch : 64, Loss : 2.1206960678100586\n",
            "Batch : 65, Loss : 2.1207191944122314\n",
            "Batch : 66, Loss : 2.0598037242889404\n",
            "Batch : 67, Loss : 2.088256359100342\n",
            "Batch : 68, Loss : 2.083505868911743\n",
            "Batch : 69, Loss : 2.0621414184570312\n",
            "Batch : 70, Loss : 2.07832407951355\n",
            "Batch : 71, Loss : 2.049839735031128\n",
            "Batch : 72, Loss : 2.101393699645996\n",
            "Batch : 73, Loss : 2.0119125843048096\n",
            "Batch : 74, Loss : 2.0472755432128906\n",
            "Batch : 75, Loss : 2.0591607093811035\n",
            "Batch : 76, Loss : 2.0145461559295654\n",
            "Batch : 77, Loss : 2.0132009983062744\n",
            "Batch : 78, Loss : 2.0692832469940186\n",
            "Batch : 79, Loss : 1.9742687940597534\n",
            "Batch : 80, Loss : 2.001490592956543\n",
            "Batch : 81, Loss : 2.0196759700775146\n",
            "Batch : 82, Loss : 2.000135660171509\n",
            "Batch : 83, Loss : 1.9519867897033691\n",
            "Batch : 84, Loss : 1.9761892557144165\n",
            "Batch : 85, Loss : 1.985669732093811\n",
            "Batch : 86, Loss : 1.9579249620437622\n",
            "Batch : 87, Loss : 1.9509353637695312\n",
            "Batch : 88, Loss : 1.9740544557571411\n",
            "Batch : 89, Loss : 1.9976918697357178\n",
            "Batch : 90, Loss : 1.9830397367477417\n",
            "Batch : 91, Loss : 1.9519944190979004\n",
            "Batch : 92, Loss : 1.9958202838897705\n",
            "Batch : 93, Loss : 1.8829996585845947\n",
            "Batch : 94, Loss : 1.9355738162994385\n",
            "Batch : 95, Loss : 1.9342906475067139\n",
            "Batch : 96, Loss : 1.9507646560668945\n",
            "Batch : 97, Loss : 1.9132877588272095\n",
            "Batch : 98, Loss : 1.8462486267089844\n",
            "Batch : 99, Loss : 1.9494094848632812\n",
            "Batch : 100, Loss : 1.9408905506134033\n",
            "Batch : 101, Loss : 1.89078950881958\n",
            "Batch : 102, Loss : 1.86064875125885\n",
            "Batch : 103, Loss : 1.9013149738311768\n",
            "Batch : 104, Loss : 1.8886622190475464\n",
            "Batch : 105, Loss : 1.8175115585327148\n",
            "Batch : 106, Loss : 1.8422091007232666\n",
            "Batch : 107, Loss : 1.8924256563186646\n",
            "Batch : 108, Loss : 1.8633731603622437\n",
            "Batch : 109, Loss : 1.8070504665374756\n",
            "Batch : 110, Loss : 1.8510191440582275\n",
            "Batch : 111, Loss : 1.8639918565750122\n",
            "Batch : 112, Loss : 1.872497320175171\n",
            "Batch : 113, Loss : 1.7743204832077026\n",
            "Batch : 114, Loss : 1.7906206846237183\n",
            "Batch : 115, Loss : 1.7556565999984741\n",
            "Batch : 116, Loss : 1.7999989986419678\n",
            "Batch : 117, Loss : 1.8129624128341675\n",
            "Batch : 118, Loss : 1.7571896314620972\n",
            "Batch : 119, Loss : 1.7813113927841187\n",
            "Batch : 120, Loss : 1.7196128368377686\n",
            "Batch : 121, Loss : 1.796372890472412\n",
            "Batch : 122, Loss : 1.826071858406067\n",
            "Batch : 123, Loss : 1.7011802196502686\n",
            "Batch : 124, Loss : 1.808985710144043\n",
            "Batch : 125, Loss : 1.7685099840164185\n",
            "Batch : 126, Loss : 1.6843271255493164\n",
            "Batch : 127, Loss : 1.752637267112732\n",
            "Batch : 128, Loss : 1.6819640398025513\n",
            "Batch : 129, Loss : 1.6977425813674927\n",
            "Batch : 130, Loss : 1.676661729812622\n",
            "Batch : 131, Loss : 1.7065660953521729\n",
            "Batch : 132, Loss : 1.682221531867981\n",
            "Batch : 133, Loss : 1.640511393547058\n",
            "Batch : 134, Loss : 1.6830825805664062\n",
            "Batch : 135, Loss : 1.752113938331604\n",
            "Batch : 136, Loss : 1.720567226409912\n",
            "Batch : 137, Loss : 1.615024447441101\n",
            "Batch : 138, Loss : 1.6726958751678467\n",
            "Batch : 139, Loss : 1.6757009029388428\n",
            "Batch : 140, Loss : 1.6010380983352661\n",
            "Batch : 141, Loss : 1.6483255624771118\n",
            "Batch : 142, Loss : 1.5700677633285522\n",
            "Batch : 143, Loss : 1.651198148727417\n",
            "Batch : 144, Loss : 1.571361780166626\n",
            "Batch : 145, Loss : 1.5436210632324219\n",
            "Batch : 146, Loss : 1.5437939167022705\n",
            "Batch : 147, Loss : 1.557311773300171\n",
            "Batch : 148, Loss : 1.5364137887954712\n",
            "Batch : 149, Loss : 1.5794886350631714\n",
            "Batch : 150, Loss : 1.569846749305725\n",
            "Batch : 151, Loss : 1.508810043334961\n",
            "Batch : 152, Loss : 1.601786494255066\n",
            "Batch : 153, Loss : 1.5836231708526611\n",
            "Batch : 154, Loss : 1.5391265153884888\n",
            "Batch : 155, Loss : 1.5010861158370972\n",
            "Batch : 156, Loss : 1.6004945039749146\n",
            "Batch : 157, Loss : 1.514001488685608\n",
            "Batch : 158, Loss : 1.6280196905136108\n",
            "Batch : 159, Loss : 1.536213755607605\n",
            "Batch : 160, Loss : 1.421632170677185\n",
            "Batch : 161, Loss : 1.4774019718170166\n",
            "Batch : 162, Loss : 1.4398362636566162\n",
            "Batch : 163, Loss : 1.373034954071045\n",
            "Batch : 164, Loss : 1.492945671081543\n",
            "Batch : 165, Loss : 1.5011619329452515\n",
            "Batch : 166, Loss : 1.5579118728637695\n",
            "Batch : 167, Loss : 1.4496339559555054\n",
            "Batch : 168, Loss : 1.4280097484588623\n",
            "Batch : 169, Loss : 1.414563775062561\n",
            "Batch : 170, Loss : 1.509659767150879\n",
            "Batch : 171, Loss : 1.4530062675476074\n",
            "Batch : 172, Loss : 1.5306224822998047\n",
            "Batch : 173, Loss : 1.431924819946289\n",
            "Batch : 174, Loss : 1.4023617506027222\n",
            "Batch : 175, Loss : 1.3567355871200562\n",
            "Batch : 176, Loss : 1.4077826738357544\n",
            "Batch : 177, Loss : 1.5180221796035767\n",
            "Batch : 178, Loss : 1.4292066097259521\n",
            "Batch : 179, Loss : 1.3601760864257812\n",
            "Batch : 180, Loss : 1.3425545692443848\n",
            "Batch : 181, Loss : 1.3390957117080688\n",
            "Batch : 182, Loss : 1.3646824359893799\n",
            "Batch : 183, Loss : 1.4188402891159058\n",
            "Batch : 184, Loss : 1.3942010402679443\n",
            "Batch : 185, Loss : 1.5305756330490112\n",
            "Batch : 186, Loss : 1.3057321310043335\n",
            "Batch : 187, Loss : 1.278499722480774\n",
            "Batch : 188, Loss : 1.3216826915740967\n",
            "Batch : 189, Loss : 1.3139963150024414\n",
            "Batch : 190, Loss : 1.4515082836151123\n",
            "Batch : 191, Loss : 1.3720842599868774\n",
            "Batch : 192, Loss : 1.386643648147583\n",
            "Batch : 193, Loss : 1.3850247859954834\n",
            "Batch : 194, Loss : 1.3192099332809448\n",
            "Batch : 195, Loss : 1.1870362758636475\n",
            "Batch : 196, Loss : 1.1684998273849487\n",
            "Batch : 197, Loss : 1.3810540437698364\n",
            "Batch : 198, Loss : 1.231268048286438\n",
            "Batch : 199, Loss : 1.2899638414382935\n",
            "Batch : 200, Loss : 1.2323558330535889\n",
            "Batch : 201, Loss : 1.2761675119400024\n",
            "Batch : 202, Loss : 1.3469880819320679\n",
            "Batch : 203, Loss : 1.2542074918746948\n",
            "Batch : 204, Loss : 1.2661486864089966\n",
            "Batch : 205, Loss : 1.2556395530700684\n",
            "Batch : 206, Loss : 1.3017535209655762\n",
            "Batch : 207, Loss : 1.2547807693481445\n",
            "Batch : 208, Loss : 1.2146357297897339\n",
            "Batch : 209, Loss : 1.156011939048767\n",
            "Batch : 210, Loss : 1.1934814453125\n",
            "Batch : 211, Loss : 1.3333114385604858\n",
            "Batch : 212, Loss : 1.1497979164123535\n",
            "Batch : 213, Loss : 1.2267824411392212\n",
            "Batch : 214, Loss : 1.2054336071014404\n",
            "Batch : 215, Loss : 1.1902906894683838\n",
            "Batch : 216, Loss : 1.256459355354309\n",
            "Batch : 217, Loss : 1.293016791343689\n",
            "Batch : 218, Loss : 1.2727394104003906\n",
            "Batch : 219, Loss : 1.1888401508331299\n",
            "Batch : 220, Loss : 1.2215769290924072\n",
            "Batch : 221, Loss : 1.1028565168380737\n",
            "Batch : 222, Loss : 1.1428041458129883\n",
            "Batch : 223, Loss : 1.1383593082427979\n",
            "Batch : 224, Loss : 1.1078866720199585\n",
            "Batch : 225, Loss : 1.114869236946106\n",
            "Batch : 226, Loss : 1.2556800842285156\n",
            "Batch : 227, Loss : 1.17098867893219\n",
            "Batch : 228, Loss : 1.3168967962265015\n",
            "Batch : 229, Loss : 1.202500820159912\n",
            "Batch : 230, Loss : 1.1309611797332764\n",
            "Batch : 231, Loss : 1.1437286138534546\n",
            "Batch : 232, Loss : 1.2180989980697632\n",
            "Batch : 233, Loss : 1.109735131263733\n",
            "Batch : 234, Loss : 1.1782708168029785\n",
            "Batch : 235, Loss : 1.1601829528808594\n",
            "Batch : 236, Loss : 1.0593533515930176\n",
            "Batch : 237, Loss : 1.2129329442977905\n",
            "Batch : 238, Loss : 1.134519338607788\n",
            "Batch : 239, Loss : 1.0950058698654175\n",
            "Batch : 240, Loss : 1.1448478698730469\n",
            "Batch : 241, Loss : 1.1075351238250732\n",
            "Batch : 242, Loss : 1.2724175453186035\n",
            "Batch : 243, Loss : 1.2596991062164307\n",
            "Batch : 244, Loss : 1.180365800857544\n",
            "Batch : 245, Loss : 1.0978431701660156\n",
            "Batch : 246, Loss : 1.0898332595825195\n",
            "Batch : 247, Loss : 1.0306885242462158\n",
            "Batch : 248, Loss : 1.204670786857605\n",
            "Batch : 249, Loss : 1.0197813510894775\n",
            "Batch : 250, Loss : 1.058676838874817\n",
            "Batch : 251, Loss : 1.0522862672805786\n",
            "Batch : 252, Loss : 1.0425394773483276\n",
            "Batch : 253, Loss : 1.04427969455719\n",
            "Batch : 254, Loss : 0.9958423972129822\n",
            "Batch : 255, Loss : 1.1264499425888062\n",
            "Batch : 256, Loss : 1.1264715194702148\n",
            "Batch : 257, Loss : 1.0676453113555908\n",
            "Batch : 258, Loss : 0.9820334315299988\n",
            "Batch : 259, Loss : 1.0893528461456299\n",
            "Batch : 260, Loss : 1.07201087474823\n",
            "Batch : 261, Loss : 1.0170215368270874\n",
            "Batch : 262, Loss : 1.0821914672851562\n",
            "Batch : 263, Loss : 1.1260817050933838\n",
            "Batch : 264, Loss : 1.1335537433624268\n",
            "Batch : 265, Loss : 1.0612313747406006\n",
            "Batch : 266, Loss : 1.0314620733261108\n",
            "Batch : 267, Loss : 0.9728954434394836\n",
            "Batch : 268, Loss : 1.0459610223770142\n",
            "Batch : 269, Loss : 1.1093896627426147\n",
            "Batch : 270, Loss : 0.9424105882644653\n",
            "Batch : 271, Loss : 1.0871177911758423\n",
            "Batch : 272, Loss : 1.1134209632873535\n",
            "Batch : 273, Loss : 1.1059212684631348\n",
            "Batch : 274, Loss : 0.9519581198692322\n",
            "Batch : 275, Loss : 0.9635353088378906\n",
            "Batch : 276, Loss : 1.0293534994125366\n",
            "Batch : 277, Loss : 1.1684354543685913\n",
            "Batch : 278, Loss : 1.161391019821167\n",
            "Batch : 279, Loss : 0.9835896492004395\n",
            "Batch : 280, Loss : 0.9698859453201294\n",
            "Batch : 281, Loss : 1.1427966356277466\n",
            "Batch : 282, Loss : 0.8684868216514587\n",
            "Batch : 283, Loss : 0.8392530679702759\n",
            "Batch : 284, Loss : 1.0242072343826294\n",
            "Batch : 285, Loss : 0.9342230558395386\n",
            "Batch : 286, Loss : 1.0497568845748901\n",
            "Batch : 287, Loss : 1.0773757696151733\n",
            "Batch : 288, Loss : 1.0138182640075684\n",
            "Batch : 289, Loss : 1.0757533311843872\n",
            "Batch : 290, Loss : 1.064647912979126\n",
            "Batch : 291, Loss : 0.9057958126068115\n",
            "Batch : 292, Loss : 0.9696691036224365\n",
            "Batch : 293, Loss : 1.0726704597473145\n",
            "Batch : 294, Loss : 0.8239172697067261\n",
            "Batch : 295, Loss : 0.9779520630836487\n",
            "Batch : 296, Loss : 1.0660455226898193\n",
            "Batch : 297, Loss : 0.9996625781059265\n",
            "Batch : 298, Loss : 1.0515776872634888\n",
            "Batch : 299, Loss : 0.9113433361053467\n",
            "Batch : 300, Loss : 0.9214286804199219\n",
            "Batch : 301, Loss : 0.9721064567565918\n",
            "Batch : 302, Loss : 1.0233759880065918\n",
            "Batch : 303, Loss : 0.8492702841758728\n",
            "Batch : 304, Loss : 0.8582378029823303\n",
            "Batch : 305, Loss : 0.908966600894928\n",
            "Batch : 306, Loss : 1.0528302192687988\n",
            "Batch : 307, Loss : 1.0490424633026123\n",
            "Batch : 308, Loss : 0.9199504256248474\n",
            "Batch : 309, Loss : 0.998641312122345\n",
            "Batch : 310, Loss : 0.9689504504203796\n",
            "Batch : 311, Loss : 0.9578123688697815\n",
            "Batch : 312, Loss : 0.9688906073570251\n",
            "Batch : 313, Loss : 0.8711782693862915\n",
            "Batch : 314, Loss : 0.9280992150306702\n",
            "Batch : 315, Loss : 0.9409406781196594\n",
            "Batch : 316, Loss : 0.9092831611633301\n",
            "Batch : 317, Loss : 0.9656992554664612\n",
            "Batch : 318, Loss : 0.8933562636375427\n",
            "Batch : 319, Loss : 1.0388855934143066\n",
            "Batch : 320, Loss : 0.8315597176551819\n",
            "Batch : 321, Loss : 1.1061660051345825\n",
            "Batch : 322, Loss : 1.1640617847442627\n",
            "Batch : 323, Loss : 1.0169610977172852\n",
            "Batch : 324, Loss : 0.8631824851036072\n",
            "Batch : 325, Loss : 0.9172012209892273\n",
            "Batch : 326, Loss : 0.8783221244812012\n",
            "Batch : 327, Loss : 0.8967846632003784\n",
            "Batch : 328, Loss : 0.8556743860244751\n",
            "Batch : 329, Loss : 0.867103636264801\n",
            "Batch : 330, Loss : 0.880163311958313\n",
            "Batch : 331, Loss : 0.9031375646591187\n",
            "Batch : 332, Loss : 1.000736951828003\n",
            "Batch : 333, Loss : 0.9165113568305969\n",
            "Batch : 334, Loss : 1.0124372243881226\n",
            "Batch : 335, Loss : 0.9099932312965393\n",
            "Batch : 336, Loss : 0.9085559248924255\n",
            "Batch : 337, Loss : 0.8260055184364319\n",
            "Batch : 338, Loss : 0.828255295753479\n",
            "Batch : 339, Loss : 0.9232873320579529\n",
            "Batch : 340, Loss : 0.8026235699653625\n",
            "Batch : 341, Loss : 0.8285282254219055\n",
            "Batch : 342, Loss : 0.7604895830154419\n",
            "Batch : 343, Loss : 0.8704037666320801\n",
            "Batch : 344, Loss : 0.8518706560134888\n",
            "Batch : 345, Loss : 0.7647017240524292\n",
            "Batch : 346, Loss : 0.9043608903884888\n",
            "Batch : 347, Loss : 0.8803724050521851\n",
            "Batch : 348, Loss : 0.9026997685432434\n",
            "Batch : 349, Loss : 0.8458384275436401\n",
            "Batch : 350, Loss : 0.7978869080543518\n",
            "Batch : 351, Loss : 0.8782473802566528\n",
            "Batch : 352, Loss : 0.8830233216285706\n",
            "Batch : 353, Loss : 0.8685045838356018\n",
            "Batch : 354, Loss : 0.8153383731842041\n",
            "Batch : 355, Loss : 0.7873236536979675\n",
            "Batch : 356, Loss : 0.8207448124885559\n",
            "Batch : 357, Loss : 0.8063254356384277\n",
            "Batch : 358, Loss : 0.95791095495224\n",
            "Batch : 359, Loss : 0.8937832117080688\n",
            "Batch : 360, Loss : 0.849419116973877\n",
            "Batch : 361, Loss : 0.7761297225952148\n",
            "Batch : 362, Loss : 0.9213104248046875\n",
            "Batch : 363, Loss : 0.8977513909339905\n",
            "Batch : 364, Loss : 0.8232572078704834\n",
            "Batch : 365, Loss : 0.7341628074645996\n",
            "Batch : 366, Loss : 0.9282909631729126\n",
            "Batch : 367, Loss : 0.9209468960762024\n",
            "Batch : 368, Loss : 1.0931588411331177\n",
            "Batch : 369, Loss : 0.8093119263648987\n",
            "Batch : 370, Loss : 0.8241066932678223\n",
            "Batch : 371, Loss : 0.9300177693367004\n",
            "Batch : 372, Loss : 0.9884700775146484\n",
            "Batch : 373, Loss : 0.8687797784805298\n",
            "Batch : 374, Loss : 0.9317631125450134\n",
            "Batch : 375, Loss : 1.0075786113739014\n",
            "Batch : 376, Loss : 0.7229516506195068\n",
            "Batch : 377, Loss : 0.9265730977058411\n",
            "Batch : 378, Loss : 0.7993655204772949\n",
            "Batch : 379, Loss : 0.845329999923706\n",
            "Batch : 380, Loss : 0.8845804929733276\n",
            "Batch : 381, Loss : 0.7798385620117188\n",
            "Batch : 382, Loss : 0.8440852761268616\n",
            "Batch : 383, Loss : 0.9713435769081116\n",
            "Batch : 384, Loss : 0.8520272374153137\n",
            "Batch : 385, Loss : 0.887842059135437\n",
            "Batch : 386, Loss : 0.8183233141899109\n",
            "Batch : 387, Loss : 0.8273850083351135\n",
            "Batch : 388, Loss : 0.791914701461792\n",
            "Batch : 389, Loss : 1.0851995944976807\n",
            "Batch : 390, Loss : 0.9101279377937317\n",
            "Batch : 391, Loss : 0.870756983757019\n",
            "Batch : 392, Loss : 0.8671719431877136\n",
            "Batch : 393, Loss : 0.9126567244529724\n",
            "Batch : 394, Loss : 0.7354415059089661\n",
            "Batch : 395, Loss : 0.8450894951820374\n",
            "Batch : 396, Loss : 0.8794063925743103\n",
            "Batch : 397, Loss : 0.8245608806610107\n",
            "Batch : 398, Loss : 0.7832266092300415\n",
            "Batch : 399, Loss : 0.8343052864074707\n",
            "Batch : 400, Loss : 0.8914241790771484\n",
            "Batch : 401, Loss : 0.8136109709739685\n",
            "Batch : 402, Loss : 0.803511917591095\n",
            "Batch : 403, Loss : 0.8830196261405945\n",
            "Batch : 404, Loss : 0.7964346408843994\n",
            "Batch : 405, Loss : 0.9002323150634766\n",
            "Batch : 406, Loss : 0.7158180475234985\n",
            "Batch : 407, Loss : 0.908835768699646\n",
            "Batch : 408, Loss : 0.766099214553833\n",
            "Batch : 409, Loss : 0.7814940810203552\n",
            "Batch : 410, Loss : 0.8427145481109619\n",
            "Batch : 411, Loss : 0.7641740441322327\n",
            "Batch : 412, Loss : 0.7866343855857849\n",
            "Batch : 413, Loss : 0.7474889159202576\n",
            "Batch : 414, Loss : 0.6577234864234924\n",
            "Batch : 415, Loss : 0.8479698896408081\n",
            "Batch : 416, Loss : 0.7902510762214661\n",
            "Batch : 417, Loss : 0.8061983585357666\n",
            "Batch : 418, Loss : 0.8338809609413147\n",
            "Batch : 419, Loss : 0.7854049801826477\n",
            "Batch : 420, Loss : 0.8026143312454224\n",
            "Batch : 421, Loss : 0.9775906801223755\n",
            "Batch : 422, Loss : 0.7196400761604309\n",
            "Batch : 423, Loss : 0.8527088165283203\n",
            "Batch : 424, Loss : 0.6800345778465271\n",
            "Batch : 425, Loss : 0.8208808898925781\n",
            "Batch : 426, Loss : 0.7840295433998108\n",
            "Batch : 427, Loss : 0.7291143536567688\n",
            "Batch : 428, Loss : 0.8702273964881897\n",
            "Batch : 429, Loss : 0.7279163599014282\n",
            "Batch : 430, Loss : 0.7340024709701538\n",
            "Batch : 431, Loss : 0.8575119376182556\n",
            "Batch : 432, Loss : 0.8004645705223083\n",
            "Batch : 433, Loss : 0.7386170029640198\n",
            "Batch : 434, Loss : 0.8095564842224121\n",
            "Batch : 435, Loss : 0.7776358723640442\n",
            "Batch : 436, Loss : 0.8417977690696716\n",
            "Batch : 437, Loss : 0.6801465749740601\n",
            "Batch : 438, Loss : 0.8613541722297668\n",
            "Batch : 439, Loss : 0.8385316133499146\n",
            "Batch : 440, Loss : 0.7009803652763367\n",
            "Batch : 441, Loss : 0.9775915145874023\n",
            "Batch : 442, Loss : 0.831624448299408\n",
            "Batch : 443, Loss : 0.5959689021110535\n",
            "Batch : 444, Loss : 0.8285868763923645\n",
            "Batch : 445, Loss : 0.6986521482467651\n",
            "Batch : 446, Loss : 0.6866313219070435\n",
            "Batch : 447, Loss : 0.8329967856407166\n",
            "Batch : 448, Loss : 0.951951265335083\n",
            "Batch : 449, Loss : 1.0132417678833008\n",
            "Batch : 450, Loss : 0.8763456344604492\n",
            "Batch : 451, Loss : 0.6130285263061523\n",
            "Batch : 452, Loss : 0.8224859237670898\n",
            "Batch : 453, Loss : 0.8952600359916687\n",
            "Batch : 454, Loss : 0.9910266399383545\n",
            "Batch : 455, Loss : 0.6610022187232971\n",
            "Batch : 456, Loss : 0.831779420375824\n",
            "Batch : 457, Loss : 0.7341945767402649\n",
            "Batch : 458, Loss : 0.8478075861930847\n",
            "Batch : 459, Loss : 0.7625994086265564\n",
            "Batch : 460, Loss : 0.7001126408576965\n",
            "Batch : 461, Loss : 0.9012734889984131\n",
            "Batch : 462, Loss : 0.7904383540153503\n",
            "Batch : 463, Loss : 0.7857794761657715\n",
            "Batch : 464, Loss : 0.7256597876548767\n",
            "Batch : 465, Loss : 0.6941745281219482\n",
            "Batch : 466, Loss : 0.6182289123535156\n",
            "Batch : 467, Loss : 0.5952734351158142\n",
            "Batch : 468, Loss : 0.7559870481491089\n",
            "Batch : 469, Loss : 0.8390774726867676\n",
            "Batch : 470, Loss : 0.8511415123939514\n",
            "Batch : 471, Loss : 0.7293105721473694\n",
            "Batch : 472, Loss : 0.7080397009849548\n",
            "Batch : 473, Loss : 0.7914280891418457\n",
            "Batch : 474, Loss : 0.7658291459083557\n",
            "Batch : 475, Loss : 0.7630999684333801\n",
            "Batch : 476, Loss : 0.908096969127655\n",
            "Batch : 477, Loss : 0.7091385722160339\n",
            "Batch : 478, Loss : 0.8651067018508911\n",
            "Batch : 479, Loss : 1.02985680103302\n",
            "Batch : 480, Loss : 0.7285401225090027\n",
            "Batch : 481, Loss : 0.7957257032394409\n",
            "Batch : 482, Loss : 0.6916404962539673\n",
            "Batch : 483, Loss : 0.7819362878799438\n",
            "Batch : 484, Loss : 0.795325517654419\n",
            "Batch : 485, Loss : 0.8483749628067017\n",
            "Batch : 486, Loss : 0.6455948352813721\n",
            "Batch : 487, Loss : 0.7877529859542847\n",
            "Batch : 488, Loss : 0.5788509845733643\n",
            "Batch : 489, Loss : 0.9020540118217468\n",
            "Batch : 490, Loss : 1.0048235654830933\n",
            "Batch : 491, Loss : 0.8163902163505554\n",
            "Batch : 492, Loss : 0.716948390007019\n",
            "Batch : 493, Loss : 0.962223470211029\n",
            "Batch : 494, Loss : 0.7211208939552307\n",
            "Batch : 495, Loss : 0.8202193975448608\n",
            "Batch : 496, Loss : 0.8427397012710571\n",
            "Batch : 497, Loss : 0.826693058013916\n",
            "Batch : 498, Loss : 0.7287763357162476\n",
            "Batch : 499, Loss : 0.6202297210693359\n",
            "Batch : 500, Loss : 0.6668335199356079\n",
            "Batch : 501, Loss : 0.7001526355743408\n",
            "Batch : 502, Loss : 0.7389899492263794\n",
            "Batch : 503, Loss : 0.769486665725708\n",
            "Batch : 504, Loss : 0.6881678104400635\n",
            "Batch : 505, Loss : 0.7485706210136414\n",
            "Batch : 506, Loss : 0.7178738713264465\n",
            "Batch : 507, Loss : 0.7645167708396912\n",
            "Batch : 508, Loss : 0.8180508017539978\n",
            "Batch : 509, Loss : 0.626120924949646\n",
            "Batch : 510, Loss : 0.5756910443305969\n",
            "Batch : 511, Loss : 0.6368902325630188\n",
            "Batch : 512, Loss : 0.8209592700004578\n",
            "Batch : 513, Loss : 0.6571759581565857\n",
            "Batch : 514, Loss : 0.8180026412010193\n",
            "Batch : 515, Loss : 0.8992956280708313\n",
            "Batch : 516, Loss : 0.5987404584884644\n",
            "Batch : 517, Loss : 0.7104084491729736\n",
            "Batch : 518, Loss : 0.6956363916397095\n",
            "Batch : 519, Loss : 0.7667860388755798\n",
            "Batch : 520, Loss : 0.6248888969421387\n",
            "Batch : 521, Loss : 0.7634408473968506\n",
            "Batch : 522, Loss : 0.7640219926834106\n",
            "Batch : 523, Loss : 0.6814162135124207\n",
            "Batch : 524, Loss : 0.7032015323638916\n",
            "Batch : 525, Loss : 0.8157201409339905\n",
            "Batch : 526, Loss : 0.7008312940597534\n",
            "Batch : 527, Loss : 0.732029914855957\n",
            "Batch : 528, Loss : 0.82368403673172\n",
            "Batch : 529, Loss : 0.7301868200302124\n",
            "Batch : 530, Loss : 0.8001646995544434\n",
            "Batch : 531, Loss : 0.7459145784378052\n",
            "Batch : 532, Loss : 0.6743749380111694\n",
            "Batch : 533, Loss : 0.6210869550704956\n",
            "Batch : 534, Loss : 0.6043013334274292\n",
            "Batch : 535, Loss : 0.7153838872909546\n",
            "Batch : 536, Loss : 0.7670523524284363\n",
            "Batch : 537, Loss : 0.7152341604232788\n",
            "Batch : 538, Loss : 0.7835890054702759\n",
            "Batch : 539, Loss : 0.8859148025512695\n",
            "Batch : 540, Loss : 0.599796712398529\n",
            "Batch : 541, Loss : 0.7726296186447144\n",
            "Batch : 542, Loss : 0.8073362112045288\n",
            "Batch : 543, Loss : 0.6855055093765259\n",
            "Batch : 544, Loss : 0.8178418874740601\n",
            "Batch : 545, Loss : 0.7820103168487549\n",
            "Batch : 546, Loss : 0.627511739730835\n",
            "Batch : 547, Loss : 0.6063073873519897\n",
            "Batch : 548, Loss : 0.5771123766899109\n",
            "Batch : 549, Loss : 0.7162004709243774\n",
            "Batch : 550, Loss : 0.7243426442146301\n",
            "Batch : 551, Loss : 0.6797076463699341\n",
            "Batch : 552, Loss : 0.6875433921813965\n",
            "Batch : 553, Loss : 0.6337019801139832\n",
            "Batch : 554, Loss : 0.7659164071083069\n",
            "Batch : 555, Loss : 0.5945261120796204\n",
            "Batch : 556, Loss : 0.7038359642028809\n",
            "Batch : 557, Loss : 0.5917539596557617\n",
            "Batch : 558, Loss : 0.7344502806663513\n",
            "Batch : 559, Loss : 0.7458842992782593\n",
            "Batch : 560, Loss : 0.6451851725578308\n",
            "Batch : 561, Loss : 0.7311140894889832\n",
            "Batch : 562, Loss : 0.7159156203269958\n",
            "Batch : 563, Loss : 0.5862364172935486\n",
            "Batch : 564, Loss : 0.6608167886734009\n",
            "Batch : 565, Loss : 0.7021737098693848\n",
            "Batch : 566, Loss : 0.6769098043441772\n",
            "Batch : 567, Loss : 0.5683209300041199\n",
            "Batch : 568, Loss : 0.812177300453186\n",
            "Batch : 569, Loss : 0.8261668682098389\n",
            "Batch : 570, Loss : 0.5531931519508362\n",
            "Batch : 571, Loss : 0.8176854848861694\n",
            "Batch : 572, Loss : 0.6182648539543152\n",
            "Batch : 573, Loss : 0.8576733469963074\n",
            "Batch : 574, Loss : 0.6879997849464417\n",
            "Batch : 575, Loss : 0.8025730848312378\n",
            "Batch : 576, Loss : 0.6449868083000183\n",
            "Batch : 577, Loss : 0.8955276012420654\n",
            "Batch : 578, Loss : 0.8352331519126892\n",
            "Batch : 579, Loss : 0.7227768898010254\n",
            "Batch : 580, Loss : 0.6427724957466125\n",
            "Batch : 581, Loss : 0.7413312792778015\n",
            "Batch : 582, Loss : 0.808437168598175\n",
            "Batch : 583, Loss : 0.7362138628959656\n",
            "Batch : 584, Loss : 0.7132488489151001\n",
            "Batch : 585, Loss : 0.6822912693023682\n",
            "Batch : 586, Loss : 0.8415608406066895\n",
            "Batch : 587, Loss : 0.7755258679389954\n",
            "Batch : 588, Loss : 0.6350167989730835\n",
            "Batch : 589, Loss : 0.7926372289657593\n",
            "Batch : 590, Loss : 0.6106440424919128\n",
            "Batch : 591, Loss : 0.5838843584060669\n",
            "Batch : 592, Loss : 0.6540549993515015\n",
            "Batch : 593, Loss : 0.6945089101791382\n",
            "Batch : 594, Loss : 0.7460717558860779\n",
            "Batch : 595, Loss : 0.7481560111045837\n",
            "Batch : 596, Loss : 0.6829530000686646\n",
            "Batch : 597, Loss : 0.9021952152252197\n",
            "Batch : 598, Loss : 0.6492295861244202\n",
            "Batch : 599, Loss : 0.5842876434326172\n",
            "Batch : 600, Loss : 0.6648239493370056\n",
            "Batch : 601, Loss : 0.8174788355827332\n",
            "Batch : 602, Loss : 0.5497934222221375\n",
            "Batch : 603, Loss : 0.640918493270874\n",
            "Batch : 604, Loss : 0.8022916316986084\n",
            "Batch : 605, Loss : 0.7065473794937134\n",
            "Batch : 606, Loss : 0.749301552772522\n",
            "Batch : 607, Loss : 0.5115973353385925\n",
            "Batch : 608, Loss : 0.7923837900161743\n",
            "Batch : 609, Loss : 0.7312189340591431\n",
            "Batch : 610, Loss : 0.754557192325592\n",
            "Batch : 611, Loss : 0.7428798079490662\n",
            "Batch : 612, Loss : 0.7046352624893188\n",
            "Batch : 613, Loss : 0.7516167759895325\n",
            "Batch : 614, Loss : 0.7577958106994629\n",
            "Batch : 615, Loss : 0.8686878681182861\n",
            "Batch : 616, Loss : 0.8833342790603638\n",
            "Batch : 617, Loss : 0.5211689472198486\n",
            "Batch : 618, Loss : 0.8018949031829834\n",
            "Batch : 619, Loss : 0.6854356527328491\n",
            "Batch : 620, Loss : 0.6868002414703369\n",
            "Batch : 621, Loss : 0.8388063907623291\n",
            "Batch : 622, Loss : 0.8420584797859192\n",
            "Batch : 623, Loss : 0.8180871605873108\n",
            "Batch : 624, Loss : 0.8232933878898621\n",
            "Batch : 625, Loss : 0.507201075553894\n",
            "Batch : 626, Loss : 0.7882360816001892\n",
            "Batch : 627, Loss : 0.704410195350647\n",
            "Batch : 628, Loss : 0.8804201483726501\n",
            "Batch : 629, Loss : 0.8332119584083557\n",
            "Batch : 630, Loss : 0.666909396648407\n",
            "Batch : 631, Loss : 0.6572176814079285\n",
            "Batch : 632, Loss : 0.7158742547035217\n",
            "Batch : 633, Loss : 0.6666794419288635\n",
            "Batch : 634, Loss : 0.5519992709159851\n",
            "Batch : 635, Loss : 0.6405119895935059\n",
            "Batch : 636, Loss : 0.6088730096817017\n",
            "Batch : 637, Loss : 0.5769708752632141\n",
            "Batch : 638, Loss : 0.7068250179290771\n",
            "Batch : 639, Loss : 0.6890103816986084\n",
            "Batch : 640, Loss : 0.740409791469574\n",
            "Batch : 641, Loss : 0.5795367956161499\n",
            "Batch : 642, Loss : 0.6451219916343689\n",
            "Batch : 643, Loss : 0.6608470678329468\n",
            "Batch : 644, Loss : 0.6699891686439514\n",
            "Batch : 645, Loss : 0.5862137079238892\n",
            "Batch : 646, Loss : 0.7122239470481873\n",
            "Batch : 647, Loss : 0.670333981513977\n",
            "Batch : 648, Loss : 0.5811541676521301\n",
            "Batch : 649, Loss : 0.6935374736785889\n",
            "Batch : 650, Loss : 0.7667178511619568\n",
            "Batch : 651, Loss : 0.6698852777481079\n",
            "Batch : 652, Loss : 0.5950971841812134\n",
            "Batch : 653, Loss : 0.6963964700698853\n",
            "Batch : 654, Loss : 0.6746510863304138\n",
            "Batch : 655, Loss : 0.7032648921012878\n",
            "Batch : 656, Loss : 0.7299913763999939\n",
            "Batch : 657, Loss : 0.6455275416374207\n",
            "Batch : 658, Loss : 0.8687377572059631\n",
            "Batch : 659, Loss : 0.6209215521812439\n",
            "Batch : 660, Loss : 0.6902905702590942\n",
            "Batch : 661, Loss : 0.8704493641853333\n",
            "Batch : 662, Loss : 0.516825795173645\n",
            "Batch : 663, Loss : 0.7159019112586975\n",
            "Batch : 664, Loss : 0.6938387155532837\n",
            "Batch : 665, Loss : 0.7606282830238342\n",
            "Batch : 666, Loss : 0.7405959963798523\n",
            "Batch : 667, Loss : 0.5392953157424927\n",
            "Batch : 668, Loss : 0.6231902837753296\n",
            "Batch : 669, Loss : 0.7490764856338501\n",
            "Batch : 670, Loss : 0.6375073790550232\n",
            "Batch : 671, Loss : 0.6096280217170715\n",
            "Batch : 672, Loss : 0.7419190406799316\n",
            "Batch : 673, Loss : 0.6921824812889099\n",
            "Batch : 674, Loss : 0.7241722941398621\n",
            "Batch : 675, Loss : 0.7331394553184509\n",
            "Batch : 676, Loss : 0.7153270840644836\n",
            "Batch : 677, Loss : 0.6741634607315063\n",
            "Batch : 678, Loss : 0.7108941078186035\n",
            "Batch : 679, Loss : 0.6718432903289795\n",
            "Batch : 680, Loss : 0.5246376991271973\n",
            "Batch : 681, Loss : 0.6983272433280945\n",
            "Batch : 682, Loss : 0.5820616483688354\n",
            "Batch : 683, Loss : 0.688755452632904\n",
            "Batch : 684, Loss : 0.6065516471862793\n",
            "Batch : 685, Loss : 0.7954365611076355\n",
            "Batch : 686, Loss : 0.6436338424682617\n",
            "Batch : 687, Loss : 0.5863467454910278\n",
            "Batch : 688, Loss : 0.5564534068107605\n",
            "Batch : 689, Loss : 0.5944897532463074\n",
            "Batch : 690, Loss : 0.38185232877731323\n",
            "Batch : 691, Loss : 0.6103392839431763\n",
            "Batch : 692, Loss : 0.890007495880127\n",
            "Batch : 693, Loss : 0.7483656406402588\n",
            "Batch : 694, Loss : 0.584596574306488\n",
            "Batch : 695, Loss : 0.7340463995933533\n",
            "Batch : 696, Loss : 0.7674030065536499\n",
            "Batch : 697, Loss : 0.7286680340766907\n",
            "Batch : 698, Loss : 0.7085633873939514\n",
            "Batch : 699, Loss : 0.5820867419242859\n",
            "Batch : 700, Loss : 0.7951140403747559\n",
            "Batch : 701, Loss : 0.8860410451889038\n",
            "Batch : 702, Loss : 0.6280925869941711\n",
            "Batch : 703, Loss : 0.6110891699790955\n",
            "Batch : 704, Loss : 0.6116705536842346\n",
            "Batch : 705, Loss : 0.4695519804954529\n",
            "Batch : 706, Loss : 0.7079540491104126\n",
            "Batch : 707, Loss : 0.6987289190292358\n",
            "Batch : 708, Loss : 0.6303238272666931\n",
            "Batch : 709, Loss : 0.5702558159828186\n",
            "Batch : 710, Loss : 0.8940381407737732\n",
            "Batch : 711, Loss : 0.6502242088317871\n",
            "Batch : 712, Loss : 0.7544262409210205\n",
            "Batch : 713, Loss : 0.6224913597106934\n",
            "Batch : 714, Loss : 0.5027211308479309\n",
            "Batch : 715, Loss : 0.659180223941803\n",
            "Batch : 716, Loss : 0.5005546808242798\n",
            "Batch : 717, Loss : 0.7125030159950256\n",
            "Batch : 718, Loss : 0.7807872891426086\n",
            "Batch : 719, Loss : 0.6570575833320618\n",
            "Batch : 720, Loss : 0.5399603843688965\n",
            "Batch : 721, Loss : 0.5486636161804199\n",
            "Batch : 722, Loss : 0.6594067215919495\n",
            "Batch : 723, Loss : 0.7550548911094666\n",
            "Batch : 724, Loss : 0.6161649823188782\n",
            "Batch : 725, Loss : 0.6684737205505371\n",
            "Batch : 726, Loss : 0.8046538829803467\n",
            "Batch : 727, Loss : 0.7367821931838989\n",
            "Batch : 728, Loss : 0.8029303550720215\n",
            "Batch : 729, Loss : 0.5814136266708374\n",
            "Batch : 730, Loss : 0.6981534361839294\n",
            "Batch : 731, Loss : 0.6441856026649475\n",
            "Batch : 732, Loss : 0.5782068967819214\n",
            "Batch : 733, Loss : 0.5967341661453247\n",
            "Batch : 734, Loss : 0.5849484205245972\n",
            "Batch : 735, Loss : 0.593930184841156\n",
            "Batch : 736, Loss : 0.7185739278793335\n",
            "Batch : 737, Loss : 0.525344729423523\n",
            "Batch : 738, Loss : 0.6590408682823181\n",
            "Batch : 739, Loss : 0.8018746376037598\n",
            "Batch : 740, Loss : 0.663655698299408\n",
            "Batch : 741, Loss : 0.6597386598587036\n",
            "Batch : 742, Loss : 0.7837392687797546\n",
            "Batch : 743, Loss : 0.6642881035804749\n",
            "Batch : 744, Loss : 0.7205308675765991\n",
            "Batch : 745, Loss : 0.5462654232978821\n",
            "Batch : 746, Loss : 0.6771993041038513\n",
            "Batch : 747, Loss : 0.6989880800247192\n",
            "Batch : 748, Loss : 0.5110651850700378\n",
            "Batch : 749, Loss : 0.5223183631896973\n",
            "Batch : 750, Loss : 0.5779609084129333\n",
            "Batch : 751, Loss : 0.5782846212387085\n",
            "Batch : 752, Loss : 0.7587739825248718\n",
            "Batch : 753, Loss : 0.4940829575061798\n",
            "Batch : 754, Loss : 0.5813635587692261\n",
            "Batch : 755, Loss : 0.6939497590065002\n",
            "Batch : 756, Loss : 0.6438460946083069\n",
            "Batch : 757, Loss : 0.6886669397354126\n",
            "Batch : 758, Loss : 0.7135837078094482\n",
            "Batch : 759, Loss : 0.4217689335346222\n",
            "Batch : 760, Loss : 0.48472604155540466\n",
            "Batch : 761, Loss : 0.630160391330719\n",
            "Batch : 762, Loss : 0.5916309952735901\n",
            "Batch : 763, Loss : 0.5841553211212158\n",
            "Batch : 764, Loss : 0.6086241006851196\n",
            "Batch : 765, Loss : 0.6889106035232544\n",
            "Batch : 766, Loss : 0.8334610462188721\n",
            "Batch : 767, Loss : 0.6386219263076782\n",
            "Batch : 768, Loss : 0.7175742983818054\n",
            "Batch : 769, Loss : 0.5114692449569702\n",
            "Batch : 770, Loss : 0.5145649909973145\n",
            "Batch : 771, Loss : 0.7464360594749451\n",
            "Batch : 772, Loss : 0.6495785713195801\n",
            "Batch : 773, Loss : 0.6236743330955505\n",
            "Batch : 774, Loss : 0.6967576146125793\n",
            "Batch : 775, Loss : 0.597825825214386\n",
            "Batch : 776, Loss : 0.6701169610023499\n",
            "Batch : 777, Loss : 0.8392019867897034\n",
            "Batch : 778, Loss : 0.5729364156723022\n",
            "Batch : 779, Loss : 0.5851526856422424\n",
            "Batch : 780, Loss : 0.5914047956466675\n",
            "Batch : 781, Loss : 0.6039842367172241\n",
            "Batch : 782, Loss : 0.5974860787391663\n",
            "Batch : 783, Loss : 0.5433883666992188\n",
            "Batch : 784, Loss : 0.6109407544136047\n",
            "Batch : 785, Loss : 0.8486321568489075\n",
            "Batch : 786, Loss : 0.6461566090583801\n",
            "Batch : 787, Loss : 0.6566077470779419\n",
            "Batch : 788, Loss : 0.50972980260849\n",
            "Batch : 789, Loss : 0.4910510778427124\n",
            "Batch : 790, Loss : 0.6405284404754639\n",
            "Batch : 791, Loss : 0.7737571001052856\n",
            "Batch : 792, Loss : 0.7025781273841858\n",
            "Batch : 793, Loss : 0.6989554762840271\n",
            "Batch : 794, Loss : 0.6297687292098999\n",
            "Batch : 795, Loss : 0.6231300830841064\n",
            "Batch : 796, Loss : 0.526017963886261\n",
            "Batch : 797, Loss : 0.7565920352935791\n",
            "Batch : 798, Loss : 0.7017866969108582\n",
            "Batch : 799, Loss : 0.637355625629425\n",
            "Batch : 800, Loss : 0.541215181350708\n",
            "Batch : 801, Loss : 0.8218504190444946\n",
            "Batch : 802, Loss : 0.5915486216545105\n",
            "Batch : 803, Loss : 0.6074948310852051\n",
            "Batch : 804, Loss : 0.6379871964454651\n",
            "Batch : 805, Loss : 0.6760660409927368\n",
            "Batch : 806, Loss : 0.5678563714027405\n",
            "Batch : 807, Loss : 0.645680844783783\n",
            "Batch : 808, Loss : 0.6024895310401917\n",
            "Batch : 809, Loss : 0.5116451978683472\n",
            "Batch : 810, Loss : 0.6582943797111511\n",
            "Batch : 811, Loss : 0.5963569283485413\n",
            "Batch : 812, Loss : 0.7261390089988708\n",
            "Batch : 813, Loss : 0.562009334564209\n",
            "Batch : 814, Loss : 0.9102864265441895\n",
            "Batch : 815, Loss : 0.7537614107131958\n",
            "Batch : 816, Loss : 0.592302680015564\n",
            "Batch : 817, Loss : 0.7710561156272888\n",
            "Batch : 818, Loss : 0.4782652258872986\n",
            "Batch : 819, Loss : 0.6543471813201904\n",
            "Batch : 820, Loss : 0.477725088596344\n",
            "Batch : 821, Loss : 0.622084379196167\n",
            "Batch : 822, Loss : 0.7645136117935181\n",
            "Batch : 823, Loss : 0.5709676146507263\n",
            "Batch : 824, Loss : 0.6635308861732483\n",
            "Batch : 825, Loss : 0.563019335269928\n",
            "Batch : 826, Loss : 0.5485863089561462\n",
            "Batch : 827, Loss : 0.717735230922699\n",
            "Batch : 828, Loss : 0.603314995765686\n",
            "Batch : 829, Loss : 0.5457782745361328\n",
            "Batch : 830, Loss : 0.6288341283798218\n",
            "Batch : 831, Loss : 0.8071845173835754\n",
            "Batch : 832, Loss : 0.561330258846283\n",
            "Batch : 833, Loss : 0.7292127013206482\n",
            "Batch : 834, Loss : 0.6548197269439697\n",
            "Batch : 835, Loss : 0.7052539587020874\n",
            "Batch : 836, Loss : 0.5346403121948242\n",
            "Batch : 837, Loss : 0.8283503651618958\n",
            "Batch : 838, Loss : 0.524527907371521\n",
            "Batch : 839, Loss : 0.723563551902771\n",
            "Batch : 840, Loss : 0.5900048017501831\n",
            "Batch : 841, Loss : 0.633603572845459\n",
            "Batch : 842, Loss : 0.8120560050010681\n",
            "Batch : 843, Loss : 0.7817659974098206\n",
            "Batch : 844, Loss : 0.5469846129417419\n",
            "Batch : 845, Loss : 0.5415099859237671\n",
            "Batch : 846, Loss : 0.4481259882450104\n",
            "Batch : 847, Loss : 0.573327362537384\n",
            "Batch : 848, Loss : 0.5560734272003174\n",
            "Batch : 849, Loss : 0.4601690471172333\n",
            "Batch : 850, Loss : 0.4570687711238861\n",
            "Batch : 851, Loss : 0.6381409764289856\n",
            "Batch : 852, Loss : 0.5778281092643738\n",
            "Batch : 853, Loss : 0.6395456790924072\n",
            "Batch : 854, Loss : 0.5782861709594727\n",
            "Batch : 855, Loss : 0.5401101112365723\n",
            "Batch : 856, Loss : 0.6445509791374207\n",
            "Batch : 857, Loss : 0.7745426297187805\n",
            "Batch : 858, Loss : 0.5757167339324951\n",
            "Batch : 859, Loss : 0.7396641373634338\n",
            "Batch : 860, Loss : 0.5011991858482361\n",
            "Batch : 861, Loss : 0.5731856822967529\n",
            "Batch : 862, Loss : 0.635742723941803\n",
            "Batch : 863, Loss : 0.5059729218482971\n",
            "Batch : 864, Loss : 0.6226193308830261\n",
            "Batch : 865, Loss : 0.8105807900428772\n",
            "Batch : 866, Loss : 0.6922105550765991\n",
            "Batch : 867, Loss : 0.5551710724830627\n",
            "Batch : 868, Loss : 0.7649716138839722\n",
            "Batch : 869, Loss : 0.6526684165000916\n",
            "Batch : 870, Loss : 0.7363131642341614\n",
            "Batch : 871, Loss : 0.5924937725067139\n",
            "Batch : 872, Loss : 0.5596998929977417\n",
            "Batch : 873, Loss : 0.7947709560394287\n",
            "Batch : 874, Loss : 0.5404989719390869\n",
            "Batch : 875, Loss : 0.6511063575744629\n",
            "Batch : 876, Loss : 0.5779381990432739\n",
            "Batch : 877, Loss : 0.5095105767250061\n",
            "Batch : 878, Loss : 0.7588096261024475\n",
            "Batch : 879, Loss : 0.72026127576828\n",
            "Batch : 880, Loss : 0.6313339471817017\n",
            "Batch : 881, Loss : 0.6880342364311218\n",
            "Batch : 882, Loss : 0.7235313057899475\n",
            "Batch : 883, Loss : 0.5517569780349731\n",
            "Batch : 884, Loss : 0.40508633852005005\n",
            "Batch : 885, Loss : 0.6411881446838379\n",
            "Batch : 886, Loss : 0.40347620844841003\n",
            "Batch : 887, Loss : 0.7129408121109009\n",
            "Batch : 888, Loss : 0.5143832564353943\n",
            "Batch : 889, Loss : 0.5083675384521484\n",
            "Batch : 890, Loss : 0.4542849361896515\n",
            "Batch : 891, Loss : 0.6496502757072449\n",
            "Batch : 892, Loss : 0.7080377340316772\n",
            "Batch : 893, Loss : 0.6833975315093994\n",
            "Batch : 894, Loss : 0.5432413816452026\n",
            "Batch : 895, Loss : 0.6883057355880737\n",
            "Batch : 896, Loss : 0.6122094988822937\n",
            "Batch : 897, Loss : 0.4388301968574524\n",
            "Batch : 898, Loss : 0.8522419929504395\n",
            "Batch : 899, Loss : 0.4930226504802704\n",
            "Batch : 900, Loss : 0.7273074388504028\n",
            "Batch : 901, Loss : 0.6783382892608643\n",
            "Batch : 902, Loss : 0.7158066034317017\n",
            "Batch : 903, Loss : 0.6768829226493835\n",
            "Batch : 904, Loss : 0.7095690965652466\n",
            "Batch : 905, Loss : 0.6534942984580994\n",
            "Batch : 906, Loss : 0.5898810625076294\n",
            "Batch : 907, Loss : 0.6722137331962585\n",
            "Batch : 908, Loss : 0.6191288232803345\n",
            "Batch : 909, Loss : 0.6187414526939392\n",
            "Batch : 910, Loss : 0.5079891085624695\n",
            "Batch : 911, Loss : 0.6107450127601624\n",
            "Batch : 912, Loss : 0.6447631120681763\n",
            "Batch : 913, Loss : 0.6354076266288757\n",
            "Batch : 914, Loss : 0.723959743976593\n",
            "Batch : 915, Loss : 0.7408987879753113\n",
            "Batch : 916, Loss : 0.588999330997467\n",
            "Batch : 917, Loss : 0.6541135907173157\n",
            "Batch : 918, Loss : 0.4577906131744385\n",
            "Batch : 919, Loss : 0.7760235071182251\n",
            "Batch : 920, Loss : 0.5031591057777405\n",
            "Batch : 921, Loss : 0.6730391383171082\n",
            "Batch : 922, Loss : 0.5100648403167725\n",
            "Batch : 923, Loss : 0.7447877526283264\n",
            "Batch : 924, Loss : 0.5182098746299744\n",
            "Batch : 925, Loss : 0.5732401609420776\n",
            "Batch : 926, Loss : 0.588056743144989\n",
            "Batch : 927, Loss : 0.6937143802642822\n",
            "Batch : 928, Loss : 0.47743600606918335\n",
            "Batch : 929, Loss : 0.5379397869110107\n",
            "Batch : 930, Loss : 0.6132214069366455\n",
            "Batch : 931, Loss : 0.543550431728363\n",
            "Batch : 932, Loss : 0.7876315116882324\n",
            "Batch : 933, Loss : 0.515562891960144\n",
            "Batch : 934, Loss : 0.6729758977890015\n",
            "Batch : 935, Loss : 0.5816141366958618\n",
            "Batch : 936, Loss : 0.5269861221313477\n",
            "Batch : 937, Loss : 0.5810511112213135\n",
            "Batch : 938, Loss : 0.6931585073471069\n",
            "Training loss: 1.0026942025433216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OVDFUnzFGpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWf1SWuiFGmn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db97148f-0b82-491d-a269-389ae33aef8a"
      },
      "source": [
        "60000/64"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "937.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ_QUMXLFGjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}